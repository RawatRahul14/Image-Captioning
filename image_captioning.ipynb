{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Captions\n",
    "\n",
    "We have 4 captioins for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34286</th>\n",
       "      <td>3683185795_704f445bf4.jpg</td>\n",
       "      <td>Boy and girl look on a puppy climbs tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8438</th>\n",
       "      <td>238512430_30dc12b683.jpg</td>\n",
       "      <td>A person sits on the front deck of a ship and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063</th>\n",
       "      <td>197504190_fd1fc3d4b7.jpg</td>\n",
       "      <td>Two children play soccer in the park .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17501</th>\n",
       "      <td>2950637275_98f1e30cca.jpg</td>\n",
       "      <td>A man doing a handstand outside of a garage .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13119</th>\n",
       "      <td>2677656448_6b7e7702af.jpg</td>\n",
       "      <td>The small brown and white dog is in the pool .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>2223382277_9efa58ec45.jpg</td>\n",
       "      <td>Two children are playing hockey on a frozen pond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626</th>\n",
       "      <td>3252588185_3210fe94be.jpg</td>\n",
       "      <td>A woman in a white shirt and dark jacket stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24145</th>\n",
       "      <td>3270273940_61ef506f05.jpg</td>\n",
       "      <td>Children jumping off of cement .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23013</th>\n",
       "      <td>3225998968_ef786d86e0.jpg</td>\n",
       "      <td>A skier in red pants is on a snow covered slope .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>1417882092_c94c251eb3.jpg</td>\n",
       "      <td>Two guys standing side by side .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "34286  3683185795_704f445bf4.jpg   \n",
       "8438    238512430_30dc12b683.jpg   \n",
       "4063    197504190_fd1fc3d4b7.jpg   \n",
       "17501  2950637275_98f1e30cca.jpg   \n",
       "13119  2677656448_6b7e7702af.jpg   \n",
       "6302   2223382277_9efa58ec45.jpg   \n",
       "23626  3252588185_3210fe94be.jpg   \n",
       "24145  3270273940_61ef506f05.jpg   \n",
       "23013  3225998968_ef786d86e0.jpg   \n",
       "2105   1417882092_c94c251eb3.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "34286           Boy and girl look on a puppy climbs tree  \n",
       "8438   A person sits on the front deck of a ship and ...  \n",
       "4063              Two children play soccer in the park .  \n",
       "17501      A man doing a handstand outside of a garage .  \n",
       "13119     The small brown and white dog is in the pool .  \n",
       "6302    Two children are playing hockey on a frozen pond  \n",
       "23626  A woman in a white shirt and dark jacket stand...  \n",
       "24145                   Children jumping off of cement .  \n",
       "23013  A skier in red pants is on a snow covered slope .  \n",
       "2105                    Two guys standing side by side .  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_cap = pd.read_csv(\"Dataset/flickr8k/captions.txt\")\n",
    "img_cap.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the tokenizer and initializing the Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary for captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in img_cap[\"caption\"].tolist():\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "vocab = vocab(counter, min_freq = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the special tokens and setting the default index as UNK (unknown token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "vocab.insert_token(unk_token, 0)\n",
    "vocab.insert_token(pad_token, 1)\n",
    "vocab.insert_token(sos_token, 2)\n",
    "vocab.insert_token(eos_token, 3)\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, defining the dataset and the creating a get_item method that returns image with their captions tokens in integer form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "\n",
    "    # Initializing the paramters for the dataset\n",
    "    def __init__(self, root_dir, captions_file, vocab, transform = None):\n",
    "\n",
    "        \"\"\"\n",
    "        root_dir: Path to the images folder\n",
    "        captions_file: Path to the CSV file containing image names and captions\n",
    "        vocab: Vocabulary object\n",
    "        transform: Optional transform to be applied on the images\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Reading the captions file and storing the image names and captions in the DataFrame df\n",
    "        df = pd.read_csv(captions_file)\n",
    "\n",
    "        self.length = len(df)\n",
    "\n",
    "        # Extracting the image names and captions from the DataFrame df\n",
    "        self.captions = df[\"caption\"]\n",
    "        self.img_names = df[\"image\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Getting caption and the image for the specified index\n",
    "        caption = self.captions[idx]\n",
    "        img_name = self.img_names[idx]\n",
    "\n",
    "        # Loading the image and applying the transform if provided\n",
    "        img_location = os.path.join(self.root_dir, img_name)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Tokenizing the caption and converting it to a list of indices in the vocabulary\n",
    "        caption_text_to_index = lambda x: [self.vocab[token] for token in tokenizer(x)]\n",
    "\n",
    "        # Adding the start and end tokens to the caption vector and converting it to a PyTorch tensor\n",
    "        caption_vec = []\n",
    "        caption_vec += [vocab[\"<sos>\"]]\n",
    "        caption_vec += caption_text_to_index(caption)\n",
    "        caption_vec += [vocab[\"<eos>\"]]\n",
    "\n",
    "        return img, torch.tensor(caption_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
